{
    "contents" : "desnconv<-function(x){\n  100*x/3861.022\n}\nlibrary(secr)\nlibrary(secrdesign)\nlibrary(scrbook)\nBearParallelModelFit<-function(trapgrid=\"../data/detectorfileScaled.csv\", subtype=NULL, iteration=1, size=450)\n\n{\n  output<-subtype\nlibrary(foreach)\n#library(doMC)\nlibrary(doParallel)\n#proxdata path and caphist\nif(size==450){proxdata<-paste(\"../data/SubsamplingData/\", subtype, \"/TempCaphist.csv\", sep=\"\")}\nif(size!=450){proxdata<-paste(\"../data/SizeData/\", subtype, \"/TempCaphist.csv\", sep=\"\")}\n  \n    bearCH<-read.capthist(captfile = proxdata, trapgrid,  detector= 'proximity', covnames=\"Sex\")\n\n#models to run within the foreach loop\nmodels<-list(list(g0~b+t+Sex, sigma~Sex), list(g0~t+Sex, sigma~Sex), list(g0~b+t, sigma~Sex), list(g0~t, sigma~Sex), list(g0~b, sigma~Sex))\nSecrFits<-vector(\"list\", 5)\nwhichMod<-c(\"Model A\", \"Model B\", \"Model C\", \"Model D\", \"Model E\")\nnames(SecrFits)<-whichMod\n\n#setup parallel backend to use all processors - note that this is not generally recommended for\n#computers that are actually in use, but since this was run primarily on a server and/or broken\n#laptop, I opted to use all cores, because I didn't need any cores to run other tasks. \ncl<-makeCluster(detectCores())\nregisterDoParallel(cl)\n\n#start time\nstrt<-Sys.time()\n\n#foreach loop, parallel (%do% operator runs systematically, %dopar% is parallel)\nforeach(d=1:5) %dopar% {\n  #import packages, need to be loaded on each processor independently \n  library(foreach)\n  library(doParallel)\n  library(secr)\n  library(secrdesign)\n  library(scrbook)\n  source('~/Google Drive/spatialMR/Rscripts/BearParallelModelFit.R')\n  secrNew<-NULL\n  modelEval<-models[[d]]\n  secrNew <- secr.fit(bearCH, model=modelEval, buffer = 10, trace = FALSE, CL=TRUE)\n  \n  if (!(is.null(secrNew)))\n  { print(dN<-derived(secrNew))\n  ##pathNew<-paste(\"../data/SubsamplingData/\", output,\"/\", whichMod[d],\"/\",output, whichMod[d],\".csv\", sep=\"\")\n  ##This was the old method of saving values from secr. I am going to keep the paths for the objects,\n  ##but save all of the data in a single csv file with three additional categorical var,\n  ##the model letter, the subtype, and the subsample ID. \n  \n    if(size==450){\n  pathNew<-(\"../data/SubsamplingData/FinalBearEst.csv\")\n  nrowSub<- nrow(read.csv(\"../data/SubsamplingData/SubsampleLabID.csv\"))\n    }\n    \n    if(size!=450){\n  pathNew<-(\"../data/SizeData/FinalBearEst.csv\")\n  nrowSub<- nrow(read.csv(\"../data/SizeData/SubsampleLabID.csv\"))\n    }\n    \n    \n  densNew<-desnconv(dN[2,c(1,3,4)])\n\n  \n  if(size==450){\n    write.table(c(nrowSub, densNew, subtype, whichMod[d]), file =pathNew, append = TRUE, col.names = FALSE, row.names = FALSE, sep = \",\")\n  pathNew<-paste(\"../data/SubsamplingData/\", output,\"/\", whichMod[d],\"/secr\", iteration, \".rds\", sep=\"\")\n  }\n  \n  if(size!=450){\n    write.table(c(nrowSub, densNew, subtype, whichMod[d], size), file =pathNew, append = TRUE, col.names = FALSE, row.names = FALSE, sep = \",\")\n    pathNew<-paste(\"../data/SizeData/\", output,\"/\", whichMod[d],\"/secr\", iteration, \"_\", size, \".rds\", sep=\"\")\n  }\n  secrNew$nsamps<-size\n  saveRDS(secrNew, file=pathNew)   }\n}\n\n\n#end time\nprint(Sys.time()-strt)\nprint(Sys.time())\nstopCluster(cl)\n}",
    "created" : 1460834990155.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2967737362",
    "id" : "45F46E27",
    "lastKnownWriteTime" : 1459809764,
    "path" : "~/Google Drive/spatialMR/Rscripts/BearParallelModelFit.R",
    "project_path" : "BearParallelModelFit.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_source"
}